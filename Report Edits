Formatted and organized the latex document

Introduction
To create the voting-based optimization algorithm, the team each created a sub-algorithm. The voting-based algorithm would assign a weight to each of the sub-algorithms to determine the one most likely to fool the classifier. The sub-algorithms chosen were Projected Gradient Descent Attack, Fast Gradient Sign Method, Random Forest Algorithm, One Pixel Attack, and . An image is inputted into the algorithm and based off the weights of the sub-algorithms, a modified image with the best possibility of fooling the model classifier is outputted.


Main Text
The Projected Gradient Descent (PDG) Attack algorithm starts by defining the path to the image file, the percentage of pixels to change in the image, the maximum number of iterations, and the step size for the gradient descent. The dimensions of the image are then recorded. From the determined image dimensions, the total number of pixels to change is determined by calculating one percent of the total pixels. The image is then flattened into a vector. A perturbation vector set as the same length of the image vector is initialized with all of its elements equal to zero. The loop for the PGD is performed, where the gradient of the perturbation vector is determined by its sign. The vector is updated subtracting it by the step size multiplied by the gradient. The perturbed image is found by adding the image vector and perturbation vector. The image vector is returned back to a matrix and every 100 iterations, the perturbed image is presented with the plot function. The new perturbed image is saved as a new file. 

The PGD is designed to fool machine learning models. Therefore, this algorithm was chosen as an adversarial attack to modify a given image in order to fool the model classifier and deceive the human eye. The PGD algorithm is less impenetrable than other adversarial attacks. The algorithm is created to perturb the image in the worse way possible while constrained to the designated range, making it harder to attack. Additionally, PGD attacks can be used for a variety of inputs, such as images, language, and speech. Due to the range of inputs, the PGD algorithm was successful in perturbing an image input. PGD attacks are also highly effective. The iteration process in the algorithm allows it to analyze the perturbation most likely to make the model classifier misclassify the image. The PGD attack was designed to change one percent of the pixels in a given image in order to deceive the model classifier because of its vigor, functionality, and ability to be applied to many input types.


Appendix

Testing
The PGD attack algorithm was tested against the model classifier numerous times. The perturbed images of the sunflowers were able to trick the model classifier. The perturbed images from the algorithm were unable to trick the classifier when shown an image of grass but a change in the prediction was observed. To test this, the original image was entered into the model classifier and the predicted observation was recorded. Then, the perturbed image was inputted to compare the prediction to the original image. For a picture of a dandelion, the original prediction was 99.7\% dandelion and after the perturbed image was inputted into the classifier, the prediction was .2\% dandelion. On average, there was a .1\% increase or decrease to the predicted score when shown a picture of grass. While the model classified the image correctly, a change in the prediction was observed. 

The correctness of the algorithm can be determined by a comparison of the input and output image, the optimization of the code, and its adversarial strength. The inputted image was in color and the PGD attack turns it into a black and white image. The output image is consistent, but unfortunately does not deceive the model classify. The PGD algorithm is optimal based on the assigned range, step size, and number of maximum iterations. The adversarial strength is determined by the inability to be detected, the success rate of the misclassification, and the distance between original and perturbed images. 



Complexity
Runtime complexity is dependent on a variety of factors, including size of the image, the number of maximum iterations, and the step size. It can be estimated as O(n*k*t), n is the number of pixels in the given input image, k is the number of maximum iterations, and t is the time complexity for each iteration. Calculating the gradient and creating the perturbed image within the range impacts the runtime complexity. For the iterations, the runtime complexity is O(n) because it runs through the number of pixels. Therefore, O(\begin{math} n^2 \end{math}*k*t) is the overall estimated runtime complexity for the PGD algorithm. 
The walltime complexity is dependent on hardware limitations. The PGD runs in about ten minutes due to the maximum iterations. 


Performance
The performance of the algorithm can be determined by a variety of factors. The success of the algorithm and the runtime. The runtime of the algorithm takes minutes to produce a perturbed image. It would be beneficial if the PGD algorithm ran faster. The success of the algorithm depended on whether the input image was grass or dandelions. The algorithm was able to perturb both grass and dandelion images, however, only perturbed images of dandelions were able to trick the classifier. The prediction of the model classifier did change after the perturbed images of grass were entered but not by a significant amount. Figure 1 shows the before and after images of grass using the PGD attack algorithm. The model classifier predicts with 99.8\% certainty Figure 1.a is grass and the perturbed image, Figure 1.b, there is 99.7\% certainty the image is grass. Figure 2 shows images of the dandelions before and after being inputted into the algorithm. The model classifier predicted there was a 99.7\% chance Figure 2.a was a dandelion and that there was a .2\% chance Figure 2.b was a dandelion.


Justification
Projected Gradient Descent Attacks are versatile, flexible in terms of optimization, and robust. PGD algorithms are versatile in input types because they can accept images, audio, and text. Structurally, the algorithm allows for adjustable factors to improve runtime, such as step size, range, and maximum iterations. The algorithm is robust and hard to detect. While the algorithm cannot fool the classifier from the perturbed images of grass, with more testing the algorithm could have a better chance at creating an image that would be misclassified. Additionally, there would need to be a better understanding of how the classifier determines if an image is grass to decide the most effective pixels the algorithm should change. The PGD attack algorithm was successful in deceiving the classifier with perturbed images of dandelions. It is likely the classifier looks for yellow pixels and the algorithm perturbs the image in a way the classifier cannot determine the image is a dandelion.

Ending
While the team was unable to create the majority voting optimization algorithm, the team would have created an algorithm that assigns a weight to each algorithm based off ... to decide which output image would most likely fool the classifier. Based off success, runtime, ... we believe the voting-based optimization algorithm would choose x algorithm majority of the time???????
